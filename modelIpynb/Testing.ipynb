{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QA Labor law training file"
      ],
      "metadata": {
        "id": "Wa_AHl84mUvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect drive"
      ],
      "metadata": {
        "id": "RzXTkS0-ZN5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nmtYJ01ZPJR",
        "outputId": "1200f79c-abbc-4c65-d1bd-0294d67f4326"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data functions"
      ],
      "metadata": {
        "id": "XMwJpEY6pWhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lib for preprocessing data"
      ],
      "metadata": {
        "id": "9Cfb0YVjpuql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "fCXdQWSTpryp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load data"
      ],
      "metadata": {
        "id": "8q-9EIpvpgaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_data(input_file: str) -> dict:\n",
        "    \"\"\"\n",
        "    Loads the data from the given input file.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): The path to the input file containing the data.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded data as a dictionary\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "_zp1GyHtpi08"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Clean text"
      ],
      "metadata": {
        "id": "vWfIUnebp9KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given text by removing newlines, numeric annotations, special characters, and converting it to lowercase.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "C4DrSBTwp8aG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocess data"
      ],
      "metadata": {
        "id": "DNvkSqnvqfOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the given data and extracts contexts, questions, and answers, context_raws, question_raws.\n",
        "\n",
        "    Args:\n",
        "      data (dict): The input data containing articles, paragraphs, and questions.\n",
        "\n",
        "    Returns:\n",
        "      tuple: A tuple containing three lists - contexts, questions, and answers.\n",
        "        - contexts (list): A list of preprocessed contexts.\n",
        "        - questions (list): A list of preprocessed questions.\n",
        "        - answers (list): A list of dictionaries, each containing the preprocessed answer text and its start position.\n",
        "        - context_raws (list): A list of raw contexts.\n",
        "        - question_raws (list): A list of raw questions.\n",
        "    \"\"\"\n",
        "    contexts = []\n",
        "    context_raws = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    question_raws = []\n",
        "    for dataSquad in data:\n",
        "        for article in dataSquad['data']:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                context = clean_text(paragraph['context'])\n",
        "                for qa in paragraph['qas']:\n",
        "                    question = clean_text(qa['question'])\n",
        "                    # Do ngữ liệu nhiều chỗ define thiếu is_impossible nên sẽ mặc định là False\n",
        "                    is_impossible = qa.get('is_impossible', False)\n",
        "                    if not is_impossible:\n",
        "                        for answer in qa['answers']:\n",
        "                            answer_text = clean_text(answer['text'])\n",
        "                            answer_start = answer['answer_start']\n",
        "                            contexts.append(context)\n",
        "                            context_raws.append(paragraph['context'])\n",
        "                            questions.append(question)\n",
        "                            question_raws.append(qa['question'])\n",
        "                            answers.append({\n",
        "                                'text': answer_text,\n",
        "                                'start': answer_start\n",
        "                            })\n",
        "    return contexts, questions, answers, context_raws, question_raws"
      ],
      "metadata": {
        "id": "gxZvqtbkqjPB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tokenize input"
      ],
      "metadata": {
        "id": "7FiUMnxMrEBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of texts using the word_tokenize function.\n",
        "\n",
        "    Args:\n",
        "      texts (list): A list of texts to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "      list: A list of tokenized texts.\n",
        "    \"\"\"\n",
        "    return [text.split() for text in texts]"
      ],
      "metadata": {
        "id": "rPV0RxnXrHKM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Running preprocessing data"
      ],
      "metadata": {
        "id": "w1PjIAtwrcJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './drive/MyDrive/models/qa_train.json'\n",
        "squad_data = load_all_data(data_path)\n",
        "contexts, questions, answers, context_raws, question_raws = preprocess_data(squad_data)\n",
        "tokenized_contexts = tokenize_texts(contexts)\n",
        "tokenized_questions = tokenize_texts(questions)"
      ],
      "metadata": {
        "id": "dhlIe_U0rbyx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data\n",
        "# tokenized_questions[:100]"
      ],
      "metadata": {
        "id": "wnB0RBdIrl4E"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2vec model"
      ],
      "metadata": {
        "id": "k7MQB-jvrxVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install lib for model word2vec"
      ],
      "metadata": {
        "id": "yF8q3qMMr709"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "BDvpvBiar2gB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5d1341-894c-4fc3-a74c-7569043c8bf2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Lib import using word2vec"
      ],
      "metadata": {
        "id": "gtHB5OPmsEwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "F5D3tirXsLFV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load model\n",
        "\n"
      ],
      "metadata": {
        "id": "EWT58ydBsNzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_questions = gensim.models.Word2Vec.load(\"./drive/MyDrive/models/word2vec_question.model\")"
      ],
      "metadata": {
        "id": "zpOHKKvXsbus"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Function model word2vec"
      ],
      "metadata": {
        "id": "HuQx6dkDbSt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_word2vec_vector(user_question, model_word2vec):\n",
        "    words = user_question\n",
        "    word_vectors = [model_word2vec.wv[word] for word in words if word in model_word2vec.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model_word2vec.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "7QF_23bbbVqn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word2vec_scores(user_question, datas, model_word2vec):\n",
        "    query_vector = get_avg_word2vec_vector(user_question, model_word2vec)\n",
        "    scores = []\n",
        "    for data in datas:\n",
        "        data_vector = get_avg_word2vec_vector(simple_preprocess(data), model_word2vec)\n",
        "        score = np.dot(query_vector, data_vector)\n",
        "        if np.isnan(score):\n",
        "            score = 0\n",
        "        scores.append(score)\n",
        "    return np.array(scores)"
      ],
      "metadata": {
        "id": "9DnJ0W_cbbHp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm BM25 (model BM25)**bold text**"
      ],
      "metadata": {
        "id": "M5-ppA9dtQT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install lib for model BM25"
      ],
      "metadata": {
        "id": "PmYXyuxotb-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "smCXZDsftj9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac354f6e-8b82-47cc-8193-cee0c280b1b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import lib using BM25"
      ],
      "metadata": {
        "id": "uyJBfG7ft6Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "Uz7Owvqyt9Mt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create model BM25"
      ],
      "metadata": {
        "id": "Tdf8nxz7tuuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_questions = BM25Okapi(tokenized_questions)"
      ],
      "metadata": {
        "id": "3ew26_dmt1rO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. BM25 function"
      ],
      "metadata": {
        "id": "dWIri6Rcb_Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bm25_scores(user_question, bm25_model):\n",
        "    bm_25_score = bm25_model.get_scores(user_question)\n",
        "    scores = (bm_25_score - np.min(bm_25_score)) / \\\n",
        "        (np.max(bm_25_score) - np.min(bm_25_score))\n",
        "    # Thay thế giá trị nan bằng 0\n",
        "    scores = np.nan_to_num(scores)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "NhXHI7K0cBqO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_top_n_ranked_bm25(question, n=5):\n",
        "    \"\"\"\n",
        "    Returns the top n ranked contexts based on BM25 score.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to rank contexts for.\n",
        "        n (int): The number of top contexts to return. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples containing the context and its BM25 score.\n",
        "    \"\"\"\n",
        "    bm25_scores = get_bm25_scores(question.split(), bm25_questions)\n",
        "    top_n = np.argsort(bm25_scores)[::-1]\n",
        "    data = []\n",
        "    count = 0\n",
        "    for index in top_n:\n",
        "        if count == n:\n",
        "            break\n",
        "        tmp = {\n",
        "            'score': f\"{bm25_scores[index]:.4f}\",\n",
        "            'question': questions[index],\n",
        "            'answer': answers[index]\n",
        "        }\n",
        "        if tmp in data:\n",
        "            continue\n",
        "        data.append(tmp)\n",
        "        count += 1\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "jRGepA4rcFeJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Example top 5 question"
      ],
      "metadata": {
        "id": "CIZxBQ9GcUr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ques = get_top_n_ranked_bm25(\"Luật lao động gồm bao nhiêu chương\")\n",
        "for q in ques:\n",
        "  print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-eUovzacXbK",
        "outputId": "790b5b8d-8491-4c82-da89-120b5691960c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': '1.0000', 'question': 'chương 1 của bộ luật lao động việt nam 2019 gồm bao nhiêu điều', 'answer': {'text': 'chương 1 gồm từ điều 1 đến điều 6', 'start': 0}}\n",
            "{'score': '0.7752', 'question': 'bộ luật lao động việt nam 2019 có bao nhiêu chương', 'answer': {'text': 'bộ luật lao động việt nam 2019 có 17 chương', 'start': 17}}\n",
            "{'score': '0.7348', 'question': 'nội dung chính của chương vi bao gồm những gì', 'answer': {'text': 'nguyên tắc trả lương hình thức trả lương kỳ hạn trả lương tiền lương làm thêm giờ tiền lương làm việc vào ban đêm tiền lương nghỉ lễ tết và ngày nghỉ có hưởng lương tiền lương ngừng việc tiền lương cho người lao động bị tạm đình chỉ công việc mức lương tối thiểu và điều chỉnh mức lương tối thiểu thỏa thuận về tiền lương và quyền và nghĩa vụ của người sử dụng lao động và người lao động trong việc trả lương và nhận lương', 'start': 84}}\n",
            "{'score': '0.7348', 'question': 'nội dung chính của chương iv bao gồm những gì', 'answer': {'text': 'quy định về học nghề đào tạo nghề bồi dưỡng nâng cao trình độ kỹ năng nghề trách nhiệm của doanh nghiệp trong đào tạo người lao động và quyền lợi của người lao động khi tham gia các chương trình đào tạo', 'start': 84}}\n",
            "{'score': '0.7348', 'question': 'nội dung chính của chương vii bao gồm những gì', 'answer': {'text': 'nguyên tắc xử lý kỷ luật lao động các hình thức xử lý kỷ luật lao động quy trình xử lý kỷ luật lao động trách nhiệm vật chất của người lao động các trường hợp không được xử lý kỷ luật lao động và quyền và nghĩa vụ của người sử dụng lao động và người lao động trong việc xử lý kỷ luật lao động và trách nhiệm vật chất', 'start': 84}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## phoBERT"
      ],
      "metadata": {
        "id": "fdvan_HjuGbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install lib for model phoBERT"
      ],
      "metadata": {
        "id": "Vi0XQFyVuKhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets rouge_score\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yl7I418EuFs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import lib for using phoBERT"
      ],
      "metadata": {
        "id": "ZYqtIDHouyzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch"
      ],
      "metadata": {
        "id": "4lV4SG5uuWhT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Choose using model"
      ],
      "metadata": {
        "id": "F0FcLpl6vaWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './drive/MyDrive/models/phoBert_model'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "Uha2D4_MvZ3b"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Function test combine model"
      ],
      "metadata": {
        "id": "9Tv-8euDdgrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_combined_scores(user_question, datas, model_word2vec, bm25_model):\n",
        "    tokenized_query = user_question.split()\n",
        "    word2vec_scores = get_word2vec_scores(\n",
        "        tokenized_query, datas, model_word2vec)\n",
        "    bm25_scores = get_bm25_scores(tokenized_query, bm25_model)\n",
        "    word2vec_scores = (word2vec_scores - np.min(word2vec_scores)) / \\\n",
        "        (np.max(word2vec_scores) - np.min(word2vec_scores))\n",
        "    combined_scores = word2vec_scores + bm25_scores\n",
        "    # Thay thế giá trị nan bằng 0 trong combined_scores\n",
        "    combined_scores = np.nan_to_num(combined_scores)\n",
        "    return combined_scores"
      ],
      "metadata": {
        "id": "EQUt1ujmdjzq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_combined_bm25_word2vec_scores(user_question):\n",
        "    tokenized_query = user_question.split()\n",
        "    word2vec_scores = get_word2vec_scores(\n",
        "        tokenized_query, questions, word2vec_questions)\n",
        "    bm25_scores = get_bm25_scores(tokenized_query, bm25_questions)\n",
        "    word2vec_scores = (word2vec_scores - np.min(word2vec_scores)) / \\\n",
        "        (np.max(word2vec_scores) - np.min(word2vec_scores))\n",
        "    combined_scores = word2vec_scores + bm25_scores\n",
        "    # Thay thế giá trị nan bằng 0 trong combined_scores\n",
        "    combined_scores = np.nan_to_num(combined_scores)\n",
        "    if np.all(combined_scores == 0):\n",
        "        return \"Không có câu hỏi được tìm thấy\", {'text': \"Không có câu trả lời được tìm thấy\"}\n",
        "\n",
        "    best_match_idx = np.argmax(combined_scores)\n",
        "    best_question = questions[best_match_idx]\n",
        "    best_ans = answers[best_match_idx]\n",
        "    print(best_question, best_ans)\n",
        "    return best_question, best_ans"
      ],
      "metadata": {
        "id": "ZcPV-DIsdpq8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test combine word2vec and BM25"
      ],
      "metadata": {
        "id": "pZiKjOJ3dstW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_combined_bm25_word2vec_scores(\"Bảo hiểm xã hội\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f74fUeFddwjG",
        "outputId": "ef598d8c-b1e1-4fb4-dd65-f2445568dfbf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ai phải tham gia bảo hiểm xã hội bắt buộc cho người lao động {'text': 'người sử dụng lao động phải tham gia bảo hiểm xã hội bắt buộc cho người lao động', 'start': 27}\n",
            "('ai phải tham gia bảo hiểm xã hội bắt buộc cho người lao động', {'text': 'người sử dụng lao động phải tham gia bảo hiểm xã hội bắt buộc cho người lao động', 'start': 27})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matching_question(user_question, questions, context_raws, question_raws):\n",
        "    combined_scores = get_combined_scores(\n",
        "        user_question, questions, word2vec_questions, bm25_questions)\n",
        "    if np.all(combined_scores == 0):\n",
        "        return \"\", \"\", \"Không có câu hỏi được tìm thấy\"\n",
        "    best_match_idx = np.argmax(combined_scores)\n",
        "    return question_raws[best_match_idx], context_raws[best_match_idx], answers[best_match_idx]"
      ],
      "metadata": {
        "id": "VhwilItYeCaM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_answer(question):\n",
        "    best_question, match_context, ans = find_best_matching_question(\n",
        "        question, questions, context_raws, question_raws)\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        best_question, match_context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer"
      ],
      "metadata": {
        "id": "8wJXZkiMeG3L"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test predict of phoBERT model"
      ],
      "metadata": {
        "id": "ISMxgnEeeISj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_predicted_answer(\"bảo hiểm xã hội\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTRWAdpreLE_",
        "outputId": "afee5d0c-0a98-4003-b8f3-3e41b86afacf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Người sử dụng lao động phải tham gia bảo hiểm xã hội bắt buộc cho người lao động theo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question):\n",
        "    bestQuestion, contextMatch, answerMatch = find_best_matching_question(\n",
        "        question, questions, context_raws, question_raws)\n",
        "    inputs = tokenizer(bestQuestion.lower(), contextMatch, return_tensors=\"pt\",\n",
        "                       max_length=128, padding=\"max_length\", truncation=\"only_second\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0,\n",
        "                                             answer_start_index: answer_end_index + 1]\n",
        "    answer_result = tokenizer.decode(predict_answer_tokens)\n",
        "\n",
        "    if len(answerMatch['text'].strip()) > len(answer_result.strip()):\n",
        "        return answerMatch['text']\n",
        "    elif (tokenizer.decode(predict_answer_tokens) == \"\"):\n",
        "        return \"Chưa thể tìm thấy câu trả lời!\"\n",
        "    else:\n",
        "        return tokenizer.decode(predict_answer_tokens)"
      ],
      "metadata": {
        "id": "0ROO8HQwfJaW"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test combine BM25 + word2vec + phoBERT"
      ],
      "metadata": {
        "id": "GIEso4PCfMnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_answer(\"Hợp đồng thử việc\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEWHs24efQT2",
        "outputId": "88fe46b0-e9e3-41ff-b364-5d3f734acfab"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Các bên có quyền thỏa thuận về việc chấm dứt hợp đồng thử việc. 2.\n"
          ]
        }
      ]
    }
  ]
}