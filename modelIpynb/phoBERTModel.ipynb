{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwQwex2XqQyf"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "TvITVD2eqXoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "# Use model\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token"
      ],
      "metadata": {
        "id": "xdCspWvkqa7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_preprocess_squad(input_file):\n",
        "  with open(input_file, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  for dataJson in data:\n",
        "    for article in dataJson['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer = qa['answers'][0]['text'] if qa['answers'] else None\n",
        "                if answer:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "  # Kiểm tra độ dài của các cột\n",
        "  assert len(contexts) == len(questions) == len(answers)\n",
        "\n",
        "  # Tạo từ điển dữ liệu\n",
        "  dataset = {\n",
        "      'context': contexts,\n",
        "      'question': questions,\n",
        "      'answer': answers\n",
        "  }\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "OMeWNHiAqlYm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_and_preprocess_squad(\"/content/data/qa_train.json\")\n",
        "data_val = load_and_preprocess_squad(\"/content/data/eval.json\")"
      ],
      "metadata": {
        "id": "8QUWhGuWqzzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = Dataset.from_dict(dataset)\n",
        "dataset_eval = Dataset.from_dict(data_val)"
      ],
      "metadata": {
        "id": "Z67EeLJzq2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "id": "iKPhgyjSq5pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=8  # change to 16 for full training\n",
        "encoder_max_length=64\n",
        "decoder_max_length=32\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "  # tokenize the inputs and labels\n",
        "  inputs = tokenizer(batch[\"question\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  outputs = tokenizer(batch[\"answer\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs.input_ids\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\n",
        "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\n",
        "\n",
        "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
        "  # We have to make sure that the PAD token is ignored\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "\n",
        "  return batch\n",
        "\n",
        "train_data = dataset_train.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns= dataset_train.column_names\n",
        ")\n",
        "train_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "\n",
        "val_data = dataset_eval.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=dataset_eval.column_names\n",
        ")\n",
        "val_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "metadata": {
        "id": "x-4Z8Sh8q7eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EncoderDecoderModel\n",
        "\n",
        "phoBert2PhoBert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"vinai/phobert-base\", \"vinai/phobert-base\")"
      ],
      "metadata": {
        "id": "4ta1rXMjq9No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set special tokens\n",
        "phoBert2PhoBert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "phoBert2PhoBert.config.eos_token_id = tokenizer.eos_token_id\n",
        "phoBert2PhoBert.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# sensible parameters for beam search\n",
        "phoBert2PhoBert.config.vocab_size = phoBert2PhoBert.config.decoder.vocab_size\n",
        "phoBert2PhoBert.config.max_length = 64\n",
        "phoBert2PhoBert.config.min_length = 56\n",
        "phoBert2PhoBert.config.no_repeat_ngram_size = 3\n",
        "phoBert2PhoBert.config.early_stopping = True\n",
        "phoBert2PhoBert.config.length_penalty = 2.0\n",
        "phoBert2PhoBert.config.num_beams = 4"
      ],
      "metadata": {
        "id": "TSFtDCnoq_HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EncoderDecoderModel"
      ],
      "metadata": {
        "id": "9JTEtBAzrA0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load rouge for validation\n",
        "rouge = datasets.load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # all unnecessary tokens are removed\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "metadata": {
        "id": "y17h3X31rCvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set training arguments - these params are not really tuned, feel free to change\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./baseline\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=80,\n",
        "    predict_with_generate=True,\n",
        "    overwrite_output_dir=True,\n",
        ")\n",
        "\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=phoBert2PhoBert,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "nMUHo8V2rEsl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}