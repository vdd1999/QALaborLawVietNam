{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_y-T9UF9wpc",
        "outputId": "a12024bb-a473-4df1-f704-af1d0c20defb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.10/dist-packages (6.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from underthesea) (0.9.10)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.6.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install torch underthesea rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj2v6oKkJlcR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_data(input_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "TZ93_wy2pq5X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahiia4dHP6Y0"
      },
      "source": [
        "# Mục mới"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OkVGeltL31R3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "cySWbxOM6IwS"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    contexts = []\n",
        "    contextRaws = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    questionRaws = []\n",
        "    for dataSquad in data:\n",
        "      for article in dataSquad['data']:\n",
        "          for paragraph in article['paragraphs']:\n",
        "              context = clean_text(paragraph['context'])\n",
        "              for qa in paragraph['qas']:\n",
        "                  question = clean_text(qa['question'])\n",
        "                  is_impossible = qa.get('is_impossible', False)  #Do ngữ liệu nhiều chỗ define thiếu is_impossible nên sẽ mặc định là False\n",
        "                  if not is_impossible:\n",
        "                      for answer in qa['answers']:\n",
        "                          answer_text = clean_text(answer['text'])\n",
        "                          answer_start = answer['answer_start']\n",
        "                          contexts.append(context)\n",
        "                          contextRaws.append(paragraph['context'])\n",
        "                          questions.append(question)\n",
        "                          questionRaws.append(qa['question'])\n",
        "                          answers.append({\n",
        "                              'text': answer_text,\n",
        "                              'start': answer_start\n",
        "                          })\n",
        "    return contexts, questions, answers, contextRaws, questionRaws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pMIsG5kOKBRH"
      },
      "outputs": [],
      "source": [
        "def tokenize_texts(texts):\n",
        " return [text.split() for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3FwAGuM5tau0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "dyBQdxTZKDur"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/data/qa_train.json'\n",
        "squad_data = load_all_data(data_path)\n",
        "contexts, questions, answers, contextRaws, questionRaws = preprocess_data(squad_data)\n",
        "tokenized_contexts = tokenize_texts(contexts)\n",
        "tokenized_questions = tokenize_texts(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Z3kZutCtMg"
      },
      "outputs": [],
      "source": [
        "tokenized_questions[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XsvYGi49-Kq",
        "outputId": "f021a3eb-6813-4bd0-83e3-70e25aed3b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7MCniQGRRK93"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r517mQU9kMZR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "V_FWej1tRTdZ"
      },
      "outputs": [],
      "source": [
        "# Tạo mô hình Word2Vec\n",
        "model_word2vec_context = Word2Vec(vector_size=100, window=10, min_count=1, sg=1, workers=4)\n",
        "model_word2vec_question = Word2Vec(vector_size=100, window=10, min_count=1, sg=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "WEMfTd3LtvKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4e9208-a6a3-478c-e764-e434631fd362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n",
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n"
          ]
        }
      ],
      "source": [
        "# Xây dựng từ điển\n",
        "model_word2vec_context.build_vocab(tokenized_contexts)\n",
        "# Xây dựng từ điển cho câu hỏi\n",
        "model_word2vec_question.build_vocab(tokenized_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ATl10yu3NT",
        "outputId": "4f92d7b2-6d30-4204-cbb4-2ca04bf3c150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24588019, 41927800)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "model_word2vec_context.train(tokenized_contexts, total_examples=model_word2vec_context.corpus_count, epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_word2vec_question.train(tokenized_questions, total_examples=model_word2vec_question.corpus_count, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ocZOeD6TZsg",
        "outputId": "adfdced9-6ea8-47ce-9b4b-eec1e0a5f0b5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6062647, 10786800)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "clbXlmB1RZEQ"
      },
      "outputs": [],
      "source": [
        "# Lưu mô hình Word2Vec đã huấn luyện\n",
        "model_word2vec_context.save(\"/content/model_word2vec_context.model\")\n",
        "model_word2vec_question.save(\"/content/model_word2vec_question.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "mLtaWHu6QamW"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Tạo BM25 model\n",
        "bm25 = BM25Okapi(tokenized_contexts)\n",
        "bm25Questions = BM25Okapi(tokenized_questions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed\n",
        "def get_avg_word2vec_vector(user_question, model_word2vec):\n",
        "    words = user_question\n",
        "    word_vectors = [model_word2vec.wv[word] for word in words if word in model_word2vec.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model_word2vec.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "jKCTsQdITrgU"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "b5KLEP8vuACC"
      },
      "outputs": [],
      "source": [
        "# Changed\n",
        "\n",
        "def get_word2vec_scores(user_question, datas, model_word2vec):\n",
        "    query_vector = get_avg_word2vec_vector(user_question, model_word2vec)\n",
        "    scores = []\n",
        "    for data in datas:\n",
        "        data_vector = get_avg_word2vec_vector(simple_preprocess(data), model_word2vec)\n",
        "        score = np.dot(query_vector, data_vector)\n",
        "        if np.isnan(score):\n",
        "            score = 0\n",
        "        scores.append(score)\n",
        "    return np.array(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Added\n",
        "def get_bm25_scores(user_question, bm25_model):\n",
        "    bm_25_score = bm25_model.get_scores(user_question)\n",
        "    scores = (bm_25_score - np.min(bm_25_score)) / (np.max(bm_25_score) - np.min(bm_25_score))\n",
        "    # Thay thế giá trị nan bằng 0\n",
        "    scores = np.nan_to_num(scores)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "pfgzTESCUSac"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Added\n",
        "def get_combined_scores(user_question, datas, model_word2vec, bm25_model):\n",
        "    tokenized_query = user_question.split()\n",
        "    word2vec_scores = get_word2vec_scores(tokenized_query, datas, model_word2vec)\n",
        "    bm25_scores = get_bm25_scores(tokenized_query, bm25_model)\n",
        "    word2vec_scores = (word2vec_scores - np.min(word2vec_scores)) / (np.max(word2vec_scores) - np.min(word2vec_scores))\n",
        "    combined_scores = word2vec_scores + bm25_scores\n",
        "    # Thay thế giá trị nan bằng 0 trong combined_scores\n",
        "    combined_scores = np.nan_to_num(combined_scores)\n",
        "    return combined_scores"
      ],
      "metadata": {
        "id": "y8VZ03K0UVHb"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matching_question(user_question, questions, contextRaws, questionRaws):\n",
        "    combined_scores = get_combined_scores(user_question, questions, model_word2vec_question, bm25Questions)\n",
        "    if np.all(combined_scores == 0):\n",
        "        return \"Không có câu hỏi được tìm thấy\"\n",
        "    best_match_idx = np.argmax(combined_scores)\n",
        "    return questionRaws[best_match_idx], contextRaws[best_match_idx], answers[best_match_idx]\n",
        "\n",
        "user_question = \"người sử dụng lao động cần làm gì về bảo hiểm xã hội\"\n",
        "closest_questions = find_best_matching_question(user_question, questions, contextRaws, questionRaws)\n",
        "print(f\"Closest Questions: {closest_questions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glR2HPArU377",
        "outputId": "9773e5c9-92b4-4496-d540-c84b10c69eb3"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest Questions: ('Người lao động phải làm gì về bảo hiểm xã hội?', 'Điều 133. Chế độ bảo hiểm xã hội. 1. Người sử dụng lao động phải tham gia và đóng bảo hiểm xã hội cho người lao động theo quy định của pháp luật về bảo hiểm xã hội. 2. Người lao động phải tham gia và đóng bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội.', {'text': 'người lao động phải tham gia và đóng bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội', 'start': 204})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matching_context(user_question, contexts):\n",
        "    combined_scores = get_combined_scores(user_question, contexts, model_word2vec_context, bm25)\n",
        "    if np.all(combined_scores == 0):\n",
        "        return \"Không có câu hỏi được tìm thấy\"\n",
        "    best_match_idx = np.argmax(combined_scores)\n",
        "    return contexts[best_match_idx]\n",
        "\n",
        "user_question = \"Người sử dụng lao động phải làm gì về bảo hiểm xã hội\"\n",
        "closest_questions = find_best_matching_context(user_question, contexts)\n",
        "print(f\"Closest Questions: {closest_questions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8q0YGK5X8K5",
        "outputId": "79cc6b6c-f4ee-4756-9b69-34e91ba49a57"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest Questions: điều 133 chế độ bảo hiểm xã hội 1 người sử dụng lao động phải tham gia và đóng bảo hiểm xã hội cho người lao động theo quy định của pháp luật về bảo hiểm xã hội 2 người lao động phải tham gia và đóng bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch"
      ],
      "metadata": {
        "id": "N-lsQHp_KOH3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X80EKItdK8kV",
        "outputId": "644d58e9-c15f-4af3-d417-4ec774795e67"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/transformers_model1\""
      ],
      "metadata": {
        "id": "eRTsUl8lK4zX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải mô hình phoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(path)\n"
      ],
      "metadata": {
        "id": "J0o3gFFcKLTP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question):\n",
        "    bestQuestion, contextMatch, answerMatch =find_best_matching_question(question, questions, contextRaws, questionRaws)\n",
        "    inputs = tokenizer(bestQuestion.lower(), contextMatch, return_tensors=\"pt\",max_length=128, padding=\"max_length\", truncation=\"only_second\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    #print(\"Context: \" + contextMatch + \"\\n\")\n",
        "    print(\"Question \" + bestQuestion+ \"\\n\")\n",
        "    answer_result = tokenizer.decode(predict_answer_tokens)\n",
        "\n",
        "    if len(answerMatch['text'].strip()) > len(answer_result.strip()):\n",
        "        print(\"Answer: \" + answerMatch['text'] + \"\\n\")\n",
        "    elif(tokenizer.decode(predict_answer_tokens) == \"\"):\n",
        "        print(\"Answer: Chưa thể tìm thấy câu trả lời \\n\")\n",
        "    else:\n",
        "        print(\"Answer: \" + tokenizer.decode(predict_answer_tokens)+ \"\\n\")\n"
      ],
      "metadata": {
        "id": "KPQvPikPNkC1"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "    inputQuestion = input(\"Input context: \\n\")\n",
        "    if (inputQuestion.lower() == \"ok\"):\n",
        "        break\n",
        "    get_answer(inputQuestion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7qc4cxJwKUR",
        "outputId": "0375640c-98ce-4a4c-a7ed-cf94231d5c95"
      },
      "execution_count": 245,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input context: \n",
            "người sử dụng lao động có cần đảm bảo thời gian làm việc và nghỉ ngơi không\n",
            "Question Người sử dụng lao động phải bảo đảm điều gì về thời giờ làm việc, thời giờ nghỉ ngơi?\n",
            "\n",
            "Answer: việc có tính chất đặc biệt do Chính phủ quy định. 2. Người sử dụng lao động phải bảo đảm thời giờ làm việc, thời\n",
            "\n",
            "Input context: \n",
            "tiền thưởng là gì\n",
            "Question Thưởng là gì?\n",
            "\n",
            "Answer: 3. Thưởng. 1. Thưởng là số tiền hoặc tài sản hoặc bằng các hình thức khác mà người sử dụng lao động thưởng cho người lao động căn cứ vào kết quả sản xuất, kinh doanh và mức độ hoàn thành công việc của người\n",
            "\n",
            "Input context: \n",
            "phụ cấp là gì\n",
            "Question Phụ cấp là gì?\n",
            "\n",
            "Answer: Phụ cấp. 1. Phụ cấp là khoản tiền hoặc tài sản mà người sử dụng lao động trả cho người lao động ngoài tiền lương để hỗ trợ cuộc sống và cải thiện điều kiện\n",
            "\n",
            "Input context: \n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_answer(question, context, model, tokenizer):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    print(answer_start)\n",
        "    print(answer_end)\n",
        "    print(input_ids)\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6z5dN9YSQaD"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"trong thời gian thử việc, người lao động có quyền gì\"\n",
        "bestQuestion, contextMatch =find_best_matching_question(question, questions, contextRaws, questionRaws)\n",
        "\n",
        "print(\"bestQuestion\", bestQuestion)\n",
        "print(\"contextMatch\", contextMatch)\n",
        "predicted_answer = get_predicted_answer(bestQuestion, contextMatch, model, tokenizer)\n",
        "print(f\"Predicted Answer: {predicted_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xli2Uy9Idd45",
        "outputId": "66b09fa6-91b0-4313-97f0-b0138c6ca22c"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestQuestion Trong thời gian thử việc, mỗi bên có quyền gì nếu thử việc không đạt yêu cầu?\n",
            "contextMatch Trong thời gian thử việc, mỗi bên có quyền huỷ bỏ thoả thuận thử việc mà không cần báo trước và không phải bồi thường nếu thử việc không đạt yêu cầu mà hai bên đã thoả thuận. Khi việc làm thử đạt yêu cầu thì người sử dụng lao động phải giao kết hợp đồng lao động với người lao động.\n",
            "tensor(38)\n",
            "tensor(54)\n",
            "[0, 92, 790, 4365, 1176, 39221, 1395, 4, 205, 145, 10, 493, 148, 183, 1176, 49, 17, 208, 413, 24234, 1881, 114, 2, 2, 92, 790, 4365, 1176, 39221, 1395, 4, 205, 145, 10, 493, 3374, 338, 8385, 6997, 1176, 49, 64, 17, 115, 441, 71, 6, 17, 41, 10788, 311, 183, 1176, 49, 17, 208, 413, 630, 64, 82, 145, 14, 8385, 30419, 10557, 251, 49, 47, 1176, 208, 413, 630, 54, 18, 5717, 8410, 1750, 2697, 41, 574, 2902, 2288, 80, 1750, 2697, 15, 18, 1750, 15012, 10838, 2]\n",
            "Predicted Answer: thuận thử việc mà không cần báo trước và không phải bồi thường nếu thử việc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_preprocess_squad(input_file):\n",
        "  with open(input_file, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  for dataJson in data:\n",
        "    for article in dataJson['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer = qa['answers'][0]['text'] if qa['answers'] else None\n",
        "                answerStart = qa['answers'][0]['answer_start'] if qa['answers'] else None\n",
        "                if answer is None:\n",
        "                  print(question)\n",
        "                if answer is None:\n",
        "                  print(question)\n",
        "                if answer:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    if not isinstance(answerStart, (int, float)):\n",
        "                      answerStart = 0\n",
        "                    answers.append({\n",
        "                      \"text\": [answer.lower()],\n",
        "                      \"start\": [answerStart]\n",
        "                    })\n",
        "\n",
        "  # Kiểm tra độ dài của các cột\n",
        "  assert len(contexts) == len(questions) == len(answers)\n",
        "\n",
        "  # Tạo từ điển dữ liệu\n",
        "  dataset = {\n",
        "      'context': contexts,\n",
        "      'question': questions,\n",
        "      'answer': answers\n",
        "  }\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "33FOxTDt6WDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=128,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answer\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"start\"][0]\n",
        "        end_char = answer[\"start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "735MDB_66bvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Đọc dữ liệu từ file JSON\n",
        "with open('/content/data/qa_train.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "contexts = []\n",
        "questions = []\n",
        "answers = []\n",
        "for squad_data in data:\n",
        "  for article in squad_data['data']:\n",
        "      for paragraph in article['paragraphs']:\n",
        "          context = paragraph['context']\n",
        "          for qa in paragraph['qas']:\n",
        "              question = qa['question']\n",
        "              answer = qa['answers'][0]['text'] if qa['answers'] else None\n",
        "              answer_start = context.find(answer) if answer else None\n",
        "              if answer:\n",
        "                  contexts.append(context)\n",
        "                  questions.append(question)\n",
        "                  answers.append({\n",
        "                      \"text\": [answer],\n",
        "                      \"start\": [answer_start]\n",
        "                  })\n",
        "\n",
        "# Chuyển dữ liệu thành định dạng list of tuples\n",
        "data = list(zip(contexts, questions, answers))\n",
        "\n",
        "# Chia dữ liệu thành train và temp (80% train, 20% temp)\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Chia temp thành validation và test (50% validation, 50% test)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Chuyển đổi dữ liệu trở lại thành các dictionary để lưu vào file JSON\n",
        "def convert_to_dict(data):\n",
        "    contexts, questions, answers = zip(*data)\n",
        "    return {\n",
        "        'context': list(contexts),\n",
        "        'question': list(questions),\n",
        "        'answer': list(answers)\n",
        "    }\n",
        "\n",
        "train_dict = convert_to_dict(train_data)\n",
        "val_dict = convert_to_dict(val_data)\n",
        "test_dict = convert_to_dict(test_data)\n",
        "\n",
        "# Lưu dữ liệu vào các file JSON\n",
        "with open('train_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(train_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('val_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(val_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('test_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_dict, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "Smv1swz-6d60"
      },
      "execution_count": 239,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}