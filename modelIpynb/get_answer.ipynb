{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_y-T9UF9wpc",
        "outputId": "aae2d981-272d-424b-c4a5-6f951de053ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.7.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, rank_bm25, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, underthesea, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 python-crfsuite-0.9.10 rank_bm25-0.2.2 underthesea-6.8.4 underthesea-core-1.0.4\n"
          ]
        }
      ],
      "source": [
        "pip install torch underthesea rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1Bns0GTN_Uj",
        "outputId": "bcc11a21-7e07-4dd2-9c98-d368e89ea20e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj2v6oKkJlcR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_data(input_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "TZ93_wy2pq5X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahiia4dHP6Y0"
      },
      "source": [
        "# Mục mới"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OkVGeltL31R3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cySWbxOM6IwS"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    contexts = []\n",
        "    contextRaws = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    questionRaws = []\n",
        "    for dataSquad in data:\n",
        "      for article in dataSquad['data']:\n",
        "          for paragraph in article['paragraphs']:\n",
        "              context = clean_text(paragraph['context'])\n",
        "              for qa in paragraph['qas']:\n",
        "                  question = clean_text(qa['question'])\n",
        "                  is_impossible = qa.get('is_impossible', False)  #Do ngữ liệu nhiều chỗ define thiếu is_impossible nên sẽ mặc định là False\n",
        "                  if not is_impossible:\n",
        "                      for answer in qa['answers']:\n",
        "                          answer_text = clean_text(answer['text'])\n",
        "                          answer_start = answer['answer_start']\n",
        "                          contexts.append(context)\n",
        "                          contextRaws.append(paragraph['context'])\n",
        "                          questions.append(question)\n",
        "                          questionRaws.append(qa['question'])\n",
        "                          answers.append({\n",
        "                              'text': answer_text,\n",
        "                              'start': answer_start\n",
        "                          })\n",
        "    return contexts, questions, answers, contextRaws, questionRaws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pMIsG5kOKBRH"
      },
      "outputs": [],
      "source": [
        "def tokenize_texts(texts):\n",
        " return [text.split() for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3FwAGuM5tau0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dyBQdxTZKDur"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/data/qa_train.json'\n",
        "squad_data = load_all_data(data_path)\n",
        "contexts, questions, answers, contextRaws, questionRaws = preprocess_data(squad_data)\n",
        "tokenized_contexts = tokenize_texts(contexts)\n",
        "tokenized_questions = tokenize_texts(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Z3kZutCtMg"
      },
      "outputs": [],
      "source": [
        "tokenized_questions[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XsvYGi49-Kq",
        "outputId": "db45f9a5-ec72-4a3a-8823-cebd7eca1de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7MCniQGRRK93"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r517mQU9kMZR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "V_FWej1tRTdZ"
      },
      "outputs": [],
      "source": [
        "# Tạo mô hình Word2Vec\n",
        "model_word2vec_context = Word2Vec(vector_size=100, window=10, min_count=1, sg=1, workers=4)\n",
        "model_word2vec_question = Word2Vec(vector_size=100, window=10, min_count=1, sg=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WEMfTd3LtvKj"
      },
      "outputs": [],
      "source": [
        "# Xây dựng từ điển\n",
        "model_word2vec_context.build_vocab(tokenized_contexts)\n",
        "# Xây dựng từ điển cho câu hỏi\n",
        "model_word2vec_question.build_vocab(tokenized_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ATl10yu3NT",
        "outputId": "ff8a56ce-8ae4-4834-c1ec-4d2a6f794ae3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24585332, 41927800)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model_word2vec_context.train(tokenized_contexts, total_examples=model_word2vec_context.corpus_count, epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_word2vec_question.train(tokenized_questions, total_examples=model_word2vec_question.corpus_count, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ocZOeD6TZsg",
        "outputId": "2419a14a-978e-4f94-dc36-61fc4bb292eb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6060765, 10786800)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "clbXlmB1RZEQ"
      },
      "outputs": [],
      "source": [
        "# Lưu mô hình Word2Vec đã huấn luyện\n",
        "model_word2vec_context.save(\"/content/model_word2vec_context.model\")\n",
        "model_word2vec_question.save(\"/content/model_word2vec_question.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mLtaWHu6QamW"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Tạo BM25 model\n",
        "bm25 = BM25Okapi(tokenized_contexts)\n",
        "bm25Questions = BM25Okapi(tokenized_questions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed\n",
        "def get_avg_word2vec_vector(user_question, model_word2vec):\n",
        "    words = user_question\n",
        "    word_vectors = [model_word2vec.wv[word] for word in words if word in model_word2vec.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model_word2vec.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "jKCTsQdITrgU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "b5KLEP8vuACC"
      },
      "outputs": [],
      "source": [
        "# Changed\n",
        "\n",
        "def get_word2vec_scores(user_question, datas, model_word2vec):\n",
        "    query_vector = get_avg_word2vec_vector(user_question, model_word2vec)\n",
        "    scores = []\n",
        "    for data in datas:\n",
        "        data_vector = get_avg_word2vec_vector(simple_preprocess(data), model_word2vec)\n",
        "        score = np.dot(query_vector, data_vector)\n",
        "        if np.isnan(score):\n",
        "            score = 0\n",
        "        scores.append(score)\n",
        "    return np.array(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Added\n",
        "def get_bm25_scores(user_question, bm25_model):\n",
        "    bm_25_score = bm25_model.get_scores(user_question)\n",
        "    scores = (bm_25_score - np.min(bm_25_score)) / (np.max(bm_25_score) - np.min(bm_25_score))\n",
        "    # Thay thế giá trị nan bằng 0\n",
        "    scores = np.nan_to_num(scores)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "pfgzTESCUSac"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Added\n",
        "def get_combined_scores(user_question, datas, model_word2vec, bm25_model):\n",
        "    tokenized_query = user_question.split()\n",
        "    word2vec_scores = get_word2vec_scores(tokenized_query, datas, model_word2vec)\n",
        "    bm25_scores = get_bm25_scores(tokenized_query, bm25_model)\n",
        "    word2vec_scores = (word2vec_scores - np.min(word2vec_scores)) / (np.max(word2vec_scores) - np.min(word2vec_scores))\n",
        "    combined_scores = word2vec_scores + bm25_scores\n",
        "    # Thay thế giá trị nan bằng 0 trong combined_scores\n",
        "    combined_scores = np.nan_to_num(combined_scores)\n",
        "    return combined_scores"
      ],
      "metadata": {
        "id": "y8VZ03K0UVHb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matching_question(user_question, questions, contextRaws, questionRaws):\n",
        "    combined_scores = get_combined_scores(user_question, questions, model_word2vec_question, bm25Questions)\n",
        "    if np.all(combined_scores == 0):\n",
        "        return \"Không có câu hỏi được tìm thấy\"\n",
        "    best_match_idx = np.argmax(combined_scores)\n",
        "    return questionRaws[best_match_idx], contextRaws[best_match_idx], answers[best_match_idx]\n",
        "\n",
        "user_question = \"người sử dụng lao động cần làm gì về bảo hiểm xã hội\"\n",
        "closest_questions = find_best_matching_question(user_question, questions, contextRaws, questionRaws)\n",
        "print(f\"Closest Questions: {closest_questions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glR2HPArU377",
        "outputId": "63a80035-aca8-417f-889b-186ce0d9db69"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest Questions: ('Người lao động phải làm gì về bảo hiểm xã hội?', 'Điều 133. Chế độ bảo hiểm xã hội. 1. Người sử dụng lao động phải tham gia và đóng bảo hiểm xã hội cho người lao động theo quy định của pháp luật về bảo hiểm xã hội. 2. Người lao động phải tham gia và đóng bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội.', {'text': 'người lao động phải tham gia và đóng bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội', 'start': 204})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matching_context(user_question, contexts):\n",
        "    combined_scores = get_combined_scores(user_question, contexts, model_word2vec_context, bm25)\n",
        "    if np.all(combined_scores == 0):\n",
        "        return \"Không có câu hỏi được tìm thấy\"\n",
        "    best_match_idx = np.argmax(combined_scores)\n",
        "    return contexts[best_match_idx]\n",
        "\n",
        "user_question = \"Người sử dụng lao động phải làm gì về bảo hiểm xã hội\"\n",
        "closest_questions = find_best_matching_context(user_question, contexts)\n",
        "print(f\"Closest Questions: {closest_questions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8q0YGK5X8K5",
        "outputId": "86cc0ab0-794e-46c6-bbc7-f2f1549cc0b9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest Questions: điều 133 chế độ bảo hiểm xã hội 1 người sử dụng lao động phải tham gia và đóng bảo hiểm xã hội cho người lao động theo quy định của pháp luật về bảo hiểm xã hội 2 người lao động phải tham gia và đóng bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch"
      ],
      "metadata": {
        "id": "N-lsQHp_KOH3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X80EKItdK8kV",
        "outputId": "91d51034-233c-4340-e34a-edc4d5c249b8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/transformers_model1\""
      ],
      "metadata": {
        "id": "eRTsUl8lK4zX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải mô hình phoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(path)\n"
      ],
      "metadata": {
        "id": "J0o3gFFcKLTP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question):\n",
        "    bestQuestion, contextMatch, answerMatch =find_best_matching_question(question, questions, contextRaws, questionRaws)\n",
        "    inputs = tokenizer(bestQuestion.lower(), contextMatch, return_tensors=\"pt\",max_length=128, padding=\"max_length\", truncation=\"only_second\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    print(\"Context: \" + contextMatch + \"\\n\")\n",
        "    print(\"Question \" + bestQuestion+ \"\\n\")\n",
        "    answer_result = tokenizer.decode(predict_answer_tokens)\n",
        "\n",
        "    if len(answerMatch['text'].strip()) > len(answer_result.strip()):\n",
        "        print(\"Answer: \" + answerMatch['text'] + \"\\n\")\n",
        "    elif(tokenizer.decode(predict_answer_tokens) == \"\"):\n",
        "        print(\"Answer: Chưa thể tìm thấy câu trả lời \\n\")\n",
        "    else:\n",
        "        print(\"Answer: \" + tokenizer.decode(predict_answer_tokens)+ \"\\n\")\n"
      ],
      "metadata": {
        "id": "KPQvPikPNkC1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "    inputQuestion = input(\"Input context: \\n\")\n",
        "    if (inputQuestion.lower() == \"ok\"):\n",
        "        break\n",
        "    get_answer(inputQuestion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7qc4cxJwKUR",
        "outputId": "24dfb58d-bb4b-47ed-bdc1-9aba4edf56b9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input context: \n",
            "người lao động cần tham gia bao nhiêu loại bảo hiểm\n",
            "Context: Điều 268. Bảo hiểm xã hội. 1. Người sử dụng lao động phải tham gia bảo hiểm xã hội bắt buộc cho người lao động theo quy định của pháp luật. 2. Người lao động có quyền được hưởng chế độ bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội.\n",
            "\n",
            "Question Ai phải tham gia bảo hiểm xã hội bắt buộc cho người lao động?\n",
            "\n",
            "Answer: 1. Người sử dụng lao động phải tham gia bảo hiểm xã hội bắt buộc cho người lao động\n",
            "\n",
            "Input context: \n",
            "bảo hiểm xã hội có quyền lợi gì\n",
            "Context: Điều 456. Quyền và nghĩa vụ của người lao động trong việc nhận chế độ bảo hiểm xã hội. 1. Người lao động có quyền nhận chế độ bảo hiểm xã hội theo quy định của pháp luật về bảo hiểm xã hội. 2. Người lao động có nghĩa vụ tuân thủ các quy định của pháp luật về việc nhận chế độ bảo hiểm xã hội.\n",
            "\n",
            "Question Người lao động có quyền gì trong việc nhận chế độ bảo hiểm xã hội?\n",
            "\n",
            "Answer: việc nhận chế độ bảo hiểm xã hội. 1. Người lao động có quyền nhận chế độ bảo hiểm xã hội theo quy định\n",
            "\n",
            "Input context: \n",
            "thưởng là gì\n",
            "Context: Điều 103. Thưởng. 1. Thưởng là số tiền hoặc tài sản hoặc bằng các hình thức khác mà người sử dụng lao động thưởng cho người lao động căn cứ vào kết quả sản xuất, kinh doanh và mức độ hoàn thành công việc của người lao động. 2. Quy chế thưởng do người sử dụng lao động quyết định và công bố công khai tại nơi làm việc sau khi tham khảo ý kiến của tổ chức đại diện người lao động tại cơ sở đối với nơi có tổ chức đại diện người lao động tại cơ sở.\n",
            "\n",
            "Question Thưởng là gì?\n",
            "\n",
            "Answer: 3. Thưởng. 1. Thưởng là số tiền hoặc tài sản hoặc bằng các hình thức khác mà người sử dụng lao động thưởng cho người lao động căn cứ vào kết quả sản xuất, kinh doanh và mức độ hoàn thành công việc của người\n",
            "\n",
            "Input context: \n",
            "tiền thưởng là gì\n",
            "Context: Điều 103. Thưởng. 1. Thưởng là số tiền hoặc tài sản hoặc bằng các hình thức khác mà người sử dụng lao động thưởng cho người lao động căn cứ vào kết quả sản xuất, kinh doanh và mức độ hoàn thành công việc của người lao động. 2. Quy chế thưởng do người sử dụng lao động quyết định và công bố công khai tại nơi làm việc sau khi tham khảo ý kiến của tổ chức đại diện người lao động tại cơ sở đối với nơi có tổ chức đại diện người lao động tại cơ sở.\n",
            "\n",
            "Question Thưởng là gì?\n",
            "\n",
            "Answer: 3. Thưởng. 1. Thưởng là số tiền hoặc tài sản hoặc bằng các hình thức khác mà người sử dụng lao động thưởng cho người lao động căn cứ vào kết quả sản xuất, kinh doanh và mức độ hoàn thành công việc của người\n",
            "\n",
            "Input context: \n",
            "người lao động có quyên nghỉ phép không\n",
            "Context: Chương V của Bộ luật Lao động Việt Nam quy định về thời giờ làm việc, thời giờ nghỉ ngơi. Nội dung chính bao gồm các quy định về thời giờ làm việc bình thường, làm thêm giờ, thời giờ nghỉ ngơi, nghỉ giữa giờ, nghỉ hàng tuần, nghỉ hàng năm, nghỉ lễ, Tết, nghỉ việc riêng có hưởng lương, nghỉ ốm, nghỉ thai sản và các chế độ nghỉ khác.\n",
            "\n",
            "Question Người lao động có thể nghỉ phép để chăm sóc người thân không?\n",
            "\n",
            "Answer: có người lao động có thể thỏa thuận với người sử dụng lao động để nghỉ phép chăm sóc người thân\n",
            "\n",
            "Input context: \n",
            "hợp đồng thử việc tối đa bao nhiêu tháng\n",
            "Context: Điều 106. Làm thêm giờ. 1. Thời giờ làm thêm của người lao động không quá 50% số giờ làm việc bình thường trong 01 ngày; không quá 12 giờ trong 01 ngày khi làm việc vào ngày nghỉ lễ, Tết và ngày nghỉ hằng tuần; không quá 40 giờ trong 01 tháng; không quá 200 giờ trong 01 năm, trừ trường hợp đặc biệt do Chính phủ quy định được làm thêm giờ không quá 300 giờ trong 01 năm. 2. Người sử dụng lao động được sử dụng người lao động làm thêm giờ khi đáp ứng đủ các điều kiện sau đây: a) Phải được sự đồng ý của người lao động; b) Bảo đảm số giờ làm thêm của người lao động không quá 50% số giờ làm việc bình thường trong 01 ngày, trừ trường hợp quy định tại khoản 3 Điều này.\n",
            "\n",
            "Question Thời giờ làm thêm tối đa trong 01 tháng là bao nhiêu?\n",
            "\n",
            "Answer: ngày nghỉ lễ, Tết và ngày nghỉ hằng tuần; không quá 40 giờ trong\n",
            "\n",
            "Input context: \n",
            "hợp động thử việc với người lao động\n",
            "Context: Điều 483. Quyền và nghĩa vụ của người lao động trong việc đơn phương chấm dứt hợp đồng thử việc. 1. Người lao động có quyền đơn phương chấm dứt hợp đồng thử việc theo quy định của pháp luật. 2. Người lao động có nghĩa vụ thông báo cho người sử dụng lao động về việc đơn phương chấm dứt hợp đồng thử việc theo quy định của pháp luật.\n",
            "\n",
            "Question Người lao động có quyền gì trong việc đơn phương chấm dứt hợp đồng thử việc?\n",
            "\n",
            "Answer: đơn phương chấm dứt hợp đồng thử việc. 1. Người lao động có quyền đơn phương chấm dứt hợp đồng\n",
            "\n",
            "Input context: \n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_answer(question, context, model, tokenizer):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    print(answer_start)\n",
        "    print(answer_end)\n",
        "    print(input_ids)\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6z5dN9YSQaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"trong thời gian thử việc, người lao động có quyền gì\"\n",
        "bestQuestion, contextMatch =find_best_matching_question(question, questions, contextRaws, questionRaws)\n",
        "\n",
        "print(\"bestQuestion\", bestQuestion)\n",
        "print(\"contextMatch\", contextMatch)\n",
        "predicted_answer = get_predicted_answer(bestQuestion, contextMatch, model, tokenizer)\n",
        "print(f\"Predicted Answer: {predicted_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xli2Uy9Idd45",
        "outputId": "66b09fa6-91b0-4313-97f0-b0138c6ca22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestQuestion Trong thời gian thử việc, mỗi bên có quyền gì nếu thử việc không đạt yêu cầu?\n",
            "contextMatch Trong thời gian thử việc, mỗi bên có quyền huỷ bỏ thoả thuận thử việc mà không cần báo trước và không phải bồi thường nếu thử việc không đạt yêu cầu mà hai bên đã thoả thuận. Khi việc làm thử đạt yêu cầu thì người sử dụng lao động phải giao kết hợp đồng lao động với người lao động.\n",
            "tensor(38)\n",
            "tensor(54)\n",
            "[0, 92, 790, 4365, 1176, 39221, 1395, 4, 205, 145, 10, 493, 148, 183, 1176, 49, 17, 208, 413, 24234, 1881, 114, 2, 2, 92, 790, 4365, 1176, 39221, 1395, 4, 205, 145, 10, 493, 3374, 338, 8385, 6997, 1176, 49, 64, 17, 115, 441, 71, 6, 17, 41, 10788, 311, 183, 1176, 49, 17, 208, 413, 630, 64, 82, 145, 14, 8385, 30419, 10557, 251, 49, 47, 1176, 208, 413, 630, 54, 18, 5717, 8410, 1750, 2697, 41, 574, 2902, 2288, 80, 1750, 2697, 15, 18, 1750, 15012, 10838, 2]\n",
            "Predicted Answer: thuận thử việc mà không cần báo trước và không phải bồi thường nếu thử việc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_preprocess_squad(input_file):\n",
        "  with open(input_file, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  for dataJson in data:\n",
        "    for article in dataJson['data']:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer = qa['answers'][0]['text'] if qa['answers'] else None\n",
        "                answerStart = qa['answers'][0]['answer_start'] if qa['answers'] else None\n",
        "                if answer is None:\n",
        "                  print(question)\n",
        "                if answer is None:\n",
        "                  print(question)\n",
        "                if answer:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    if not isinstance(answerStart, (int, float)):\n",
        "                      answerStart = 0\n",
        "                    answers.append({\n",
        "                      \"text\": [answer.lower()],\n",
        "                      \"start\": [answerStart]\n",
        "                    })\n",
        "\n",
        "  # Kiểm tra độ dài của các cột\n",
        "  assert len(contexts) == len(questions) == len(answers)\n",
        "\n",
        "  # Tạo từ điển dữ liệu\n",
        "  dataset = {\n",
        "      'context': contexts,\n",
        "      'question': questions,\n",
        "      'answer': answers\n",
        "  }\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "33FOxTDt6WDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=128,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answer\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"start\"][0]\n",
        "        end_char = answer[\"start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "735MDB_66bvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Đọc dữ liệu từ file JSON\n",
        "with open('/content/data/qa_train.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "contexts = []\n",
        "questions = []\n",
        "answers = []\n",
        "for squad_data in data:\n",
        "  for article in squad_data['data']:\n",
        "      for paragraph in article['paragraphs']:\n",
        "          context = paragraph['context']\n",
        "          for qa in paragraph['qas']:\n",
        "              question = qa['question']\n",
        "              answer = qa['answers'][0]['text'] if qa['answers'] else None\n",
        "              answer_start = context.find(answer) if answer else None\n",
        "              if answer:\n",
        "                  contexts.append(context)\n",
        "                  questions.append(question)\n",
        "                  answers.append({\n",
        "                      \"text\": [answer],\n",
        "                      \"start\": [answer_start]\n",
        "                  })\n",
        "\n",
        "# Chuyển dữ liệu thành định dạng list of tuples\n",
        "data = list(zip(contexts, questions, answers))\n",
        "\n",
        "# Chia dữ liệu thành train và temp (80% train, 20% temp)\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Chia temp thành validation và test (50% validation, 50% test)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Chuyển đổi dữ liệu trở lại thành các dictionary để lưu vào file JSON\n",
        "def convert_to_dict(data):\n",
        "    contexts, questions, answers = zip(*data)\n",
        "    return {\n",
        "        'context': list(contexts),\n",
        "        'question': list(questions),\n",
        "        'answer': list(answers)\n",
        "    }\n",
        "\n",
        "train_dict = convert_to_dict(train_data)\n",
        "val_dict = convert_to_dict(val_data)\n",
        "test_dict = convert_to_dict(test_data)\n",
        "\n",
        "# Lưu dữ liệu vào các file JSON\n",
        "with open('train_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(train_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('val_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(val_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open('test_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_dict, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "Smv1swz-6d60"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}