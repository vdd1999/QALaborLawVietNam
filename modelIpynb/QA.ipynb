{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["XMwJpEY6pWhW","k7MQB-jvrxVg"],"authorship_tag":"ABX9TyNm4a+fdUYrJtmx4QGs+vXw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# QA Labor law training file"],"metadata":{"id":"Wa_AHl84mUvH"}},{"cell_type":"markdown","source":["## Preprocessing data functions"],"metadata":{"id":"XMwJpEY6pWhW"}},{"cell_type":"markdown","source":["* Lib for preprocessing data"],"metadata":{"id":"9Cfb0YVjpuql"}},{"cell_type":"code","source":["import json\n","import re"],"metadata":{"id":"fCXdQWSTpryp","executionInfo":{"status":"ok","timestamp":1722158746667,"user_tz":-420,"elapsed":516,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["1. Load data"],"metadata":{"id":"8q-9EIpvpgaE"}},{"cell_type":"code","source":["def load_all_data(input_file: str) -> dict:\n","    \"\"\"\n","    Loads the data from the given input file.\n","\n","    Args:\n","        input_file (str): The path to the input file containing the data.\n","\n","    Returns:\n","        dict: The loaded data as a dictionary\n","    \"\"\"\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","    return data"],"metadata":{"id":"_zp1GyHtpi08","executionInfo":{"status":"ok","timestamp":1722158766846,"user_tz":-420,"elapsed":403,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["2. Clean text"],"metadata":{"id":"vWfIUnebp9KC"}},{"cell_type":"code","source":["def clean_text(text):\n","    \"\"\"\n","    Cleans the given text by removing newlines, numeric annotations, special characters, and converting it to lowercase.\n","\n","    Args:\n","        text (str): The text to be cleaned.\n","\n","    Returns:\n","        str: The cleaned text.\n","    \"\"\"\n","    text = re.sub(r'\\n', ' ', text)\n","    text = re.sub(r'\\[\\d+\\]', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text = text.lower()\n","    return text"],"metadata":{"id":"C4DrSBTwp8aG","executionInfo":{"status":"ok","timestamp":1722158770533,"user_tz":-420,"elapsed":418,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["3. Preprocess data"],"metadata":{"id":"DNvkSqnvqfOR"}},{"cell_type":"code","source":["def preprocess_data(data):\n","    \"\"\"\n","    Preprocesses the given data and extracts contexts, questions, and answers, context_raws, question_raws.\n","\n","    Args:\n","      data (dict): The input data containing articles, paragraphs, and questions.\n","\n","    Returns:\n","      tuple: A tuple containing three lists - contexts, questions, and answers.\n","        - contexts (list): A list of preprocessed contexts.\n","        - questions (list): A list of preprocessed questions.\n","        - answers (list): A list of dictionaries, each containing the preprocessed answer text and its start position.\n","        - context_raws (list): A list of raw contexts.\n","        - question_raws (list): A list of raw questions.\n","    \"\"\"\n","    contexts = []\n","    context_raws = []\n","    questions = []\n","    answers = []\n","    question_raws = []\n","    for dataSquad in data:\n","        for article in dataSquad['data']:\n","            for paragraph in article['paragraphs']:\n","                context = clean_text(paragraph['context'])\n","                for qa in paragraph['qas']:\n","                    question = clean_text(qa['question'])\n","                    # Do ngữ liệu nhiều chỗ define thiếu is_impossible nên sẽ mặc định là False\n","                    is_impossible = qa.get('is_impossible', False)\n","                    if not is_impossible:\n","                        for answer in qa['answers']:\n","                            answer_text = clean_text(answer['text'])\n","                            answer_start = answer['answer_start']\n","                            contexts.append(context)\n","                            context_raws.append(paragraph['context'])\n","                            questions.append(question)\n","                            question_raws.append(qa['question'])\n","                            answers.append({\n","                                'text': answer_text,\n","                                'start': answer_start\n","                            })\n","    return contexts, questions, answers, context_raws, question_raws"],"metadata":{"id":"gxZvqtbkqjPB","executionInfo":{"status":"ok","timestamp":1722158890853,"user_tz":-420,"elapsed":413,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["4. Tokenize input"],"metadata":{"id":"7FiUMnxMrEBX"}},{"cell_type":"code","source":["def tokenize_texts(texts):\n","    \"\"\"\n","    Tokenizes a list of texts using the word_tokenize function.\n","\n","    Args:\n","      texts (list): A list of texts to be tokenized.\n","\n","    Returns:\n","      list: A list of tokenized texts.\n","    \"\"\"\n","    return [text.split() for text in texts]"],"metadata":{"id":"rPV0RxnXrHKM","executionInfo":{"status":"ok","timestamp":1722158983122,"user_tz":-420,"elapsed":3,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["* Running preprocessing data"],"metadata":{"id":"w1PjIAtwrcJx"}},{"cell_type":"code","source":["data_path = '/content/data/qa_train.json'\n","squad_data = load_all_data(data_path)\n","contexts, questions, answers, contextRaws, questionRaws = preprocess_data(squad_data)\n","tokenized_contexts = tokenize_texts(contexts)\n","tokenized_questions = tokenize_texts(questions)"],"metadata":{"id":"dhlIe_U0rbyx","executionInfo":{"status":"ok","timestamp":1722159080104,"user_tz":-420,"elapsed":413,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Testing data\n","# tokenized_questions[:100]"],"metadata":{"id":"wnB0RBdIrl4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word2vec model"],"metadata":{"id":"k7MQB-jvrxVg"}},{"cell_type":"markdown","source":["1. Install lib for model word2vec"],"metadata":{"id":"yF8q3qMMr709"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"BDvpvBiar2gB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Lib import using word2vec"],"metadata":{"id":"gtHB5OPmsEwg"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess"],"metadata":{"id":"F5D3tirXsLFV","executionInfo":{"status":"ok","timestamp":1722159258297,"user_tz":-420,"elapsed":2577,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["3. Create args model word2vec, build up lib for model and training"],"metadata":{"id":"EWT58ydBsNzi"}},{"cell_type":"code","source":["model_word2vec_question = Word2Vec(vector_size=100, window=10, min_count=1, sg=1, workers=4)\n","model_word2vec_question.build_vocab(tokenized_questions)\n","model_word2vec_question.train(tokenized_questions, total_examples=model_word2vec_question.corpus_count, epochs=200)\n","model_word2vec_question.save(\"/content/model_word2vec_question.model\")"],"metadata":{"id":"zpOHKKvXsbus","executionInfo":{"status":"ok","timestamp":1722159508052,"user_tz":-420,"elapsed":38099,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Algorithm BM25 (model BM25)**bold text**"],"metadata":{"id":"M5-ppA9dtQT-"}},{"cell_type":"markdown","source":["1. Install lib for model BM25"],"metadata":{"id":"PmYXyuxotb-s"}},{"cell_type":"code","source":["!pip install rank_bm25"],"metadata":{"id":"smCXZDsftj9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Import lib using BM25"],"metadata":{"id":"uyJBfG7ft6Lr"}},{"cell_type":"code","source":["from rank_bm25 import BM25Okapi"],"metadata":{"id":"Uz7Owvqyt9Mt","executionInfo":{"status":"ok","timestamp":1722159727409,"user_tz":-420,"elapsed":415,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["3. Create model BM25"],"metadata":{"id":"Tdf8nxz7tuuU"}},{"cell_type":"code","source":["bm25Questions = BM25Okapi(tokenized_questions)"],"metadata":{"id":"3ew26_dmt1rO","executionInfo":{"status":"ok","timestamp":1722159734374,"user_tz":-420,"elapsed":422,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["## phoBERT"],"metadata":{"id":"fdvan_HjuGbs"}},{"cell_type":"markdown","source":["1. Install lib for model phoBERT"],"metadata":{"id":"Vi0XQFyVuKhz"}},{"cell_type":"code","source":["!pip install datasets rouge_score\n","!pip install accelerate -U"],"metadata":{"collapsed":true,"id":"yl7I418EuFs9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Import lib using for train phoBERT"],"metadata":{"id":"ZYqtIDHouyzN"}},{"cell_type":"code","source":["import datasets\n","import transformers\n","from datasets import Dataset\n","import json\n","import torch\n","from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering, DefaultDataCollator, TrainingArguments, Trainer, AutoModelForQuestionAnswering\n","from transformers import DefaultDataCollator"],"metadata":{"id":"4lV4SG5uuWhT","executionInfo":{"status":"ok","timestamp":1722160665526,"user_tz":-420,"elapsed":414,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["3. Choose using model"],"metadata":{"id":"F0FcLpl6vaWt"}},{"cell_type":"code","source":["model = RobertaForQuestionAnswering.from_pretrained(\"vinai/phobert-base\")\n","tokenizer = RobertaTokenizerFast.from_pretrained(\"vinai/phobert-base\")"],"metadata":{"id":"Uha2D4_MvZ3b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Support function prepare dataset"],"metadata":{"id":"eUneMEICvhFw"}},{"cell_type":"markdown","source":["4.1. Load preprocess data set"],"metadata":{"id":"vsZ5uCNFvxsm"}},{"cell_type":"code","source":["def load_and_preprocess_squad(input_file):\n","  with open(input_file, 'r', encoding='utf-8') as f:\n","      data = json.load(f)\n","\n","  contexts = []\n","  questions = []\n","  answers = []\n","  for dataJson in data:\n","    for article in dataJson['data']:\n","        for paragraph in article['paragraphs']:\n","            context = paragraph['context']\n","            for qa in paragraph['qas']:\n","                question = qa['question']\n","                answer = qa['answers'][0]['text'] if qa['answers'] else None\n","                answerStart = qa['answers'][0]['answer_start'] if qa['answers'] else None\n","                if answer is None:\n","                  print(question)\n","                if answer is None:\n","                  print(question)\n","                if answer:\n","                    contexts.append(context)\n","                    questions.append(question)\n","                    answers.append({\n","                      \"text\": [answer.lower()],\n","                      \"start\": [answerStart]\n","                    })\n","\n","  # Kiểm tra độ dài của các cột\n","  assert len(contexts) == len(questions) == len(answers)\n","\n","  # Tạo từ điển dữ liệu\n","  dataset = {\n","      'context': contexts,\n","      'question': questions,\n","      'answer': answers\n","  }\n","\n","  return dataset"],"metadata":{"id":"z64E5ITLvIH2","executionInfo":{"status":"ok","timestamp":1722160180646,"user_tz":-420,"elapsed":484,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["4.2 Preprocess function"],"metadata":{"id":"hlJUGzO5wfd6"}},{"cell_type":"code","source":["def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=128,\n","        truncation=\"only_second\",\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    answers = examples[\"answer\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        answer = answers[i]\n","        start_char = answer[\"start\"][0]\n","        end_char = answer[\"start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label it (0, 0)\n","        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs\n"],"metadata":{"id":"JMAAe_Fxwh9J","executionInfo":{"status":"ok","timestamp":1722160401840,"user_tz":-420,"elapsed":515,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["5. Load train, eval data"],"metadata":{"id":"ni2QQx-MwwFm"}},{"cell_type":"code","source":["dataset = load_and_preprocess_squad(\"/content/data/qa_train.json\")\n","data_val = load_and_preprocess_squad(\"/content/data/eval.json\")\n","\n","dataset_train = Dataset.from_dict(dataset)\n","dataset_eval = Dataset.from_dict(data_val)"],"metadata":{"id":"ck5Gwn-Vwzj1","executionInfo":{"status":"ok","timestamp":1722160489395,"user_tz":-420,"elapsed":419,"user":{"displayName":"đào An","userId":"14390733647317670694"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# print dataset train\n","print(dataset_train)\n","# print dataset eval\n","print(dataset_eval)"],"metadata":{"id":"tDPswME9w4ll"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Tokenize data training, eval"],"metadata":{"id":"mjq-IAUFxPvv"}},{"cell_type":"code","source":["tokenized_squad = dataset_train.map(preprocess_function, batched=True, remove_columns=dataset_train.column_names)\n","tokenized_squad_eval = dataset_eval.map(preprocess_function, batched=True, remove_columns=dataset_eval.column_names)"],"metadata":{"id":"n-ht8q6nxS42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DefaultDataCollator"],"metadata":{"id":"DYW8ghm2xqzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Training args"],"metadata":{"id":"vnGLSBNSxpBt"}},{"cell_type":"code","source":["data_collator = DefaultDataCollator()\n","\n","training_args = TrainingArguments(\n","    output_dir=\"phobert_law\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=150,\n","    push_to_hub=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_squad,\n","    eval_dataset=tokenized_squad_eval,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"],"metadata":{"id":"T5_3Yiqzxs-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. Train model"],"metadata":{"id":"m3It5zjhx4Ck"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"bkcxmO_Zx6XO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9. Saving model"],"metadata":{"id":"LC5wqAGhyKTg"}},{"cell_type":"code","source":["trainer.save_pretrained('/content/data')\n","tokenizer.save_pretrained('/content/data')"],"metadata":{"id":"5yv1Jcd2yO-B"},"execution_count":null,"outputs":[]}]}