{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XMwJpEY6pWhW",
        "k7MQB-jvrxVg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QA Labor law training file"
      ],
      "metadata": {
        "id": "Wa_AHl84mUvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data functions"
      ],
      "metadata": {
        "id": "XMwJpEY6pWhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lib for preprocessing data"
      ],
      "metadata": {
        "id": "9Cfb0YVjpuql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "fCXdQWSTpryp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load data"
      ],
      "metadata": {
        "id": "8q-9EIpvpgaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_data(input_file: str) -> dict:\n",
        "    \"\"\"\n",
        "    Loads the data from the given input file.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): The path to the input file containing the data.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded data as a dictionary\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "_zp1GyHtpi08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Clean text"
      ],
      "metadata": {
        "id": "vWfIUnebp9KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the given text by removing newlines, numeric annotations, special characters, and converting it to lowercase.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "C4DrSBTwp8aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocess data"
      ],
      "metadata": {
        "id": "DNvkSqnvqfOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the given data and extracts contexts, questions, and answers, context_raws, question_raws.\n",
        "\n",
        "    Args:\n",
        "      data (dict): The input data containing articles, paragraphs, and questions.\n",
        "\n",
        "    Returns:\n",
        "      tuple: A tuple containing three lists - contexts, questions, and answers.\n",
        "        - contexts (list): A list of preprocessed contexts.\n",
        "        - questions (list): A list of preprocessed questions.\n",
        "        - answers (list): A list of dictionaries, each containing the preprocessed answer text and its start position.\n",
        "        - context_raws (list): A list of raw contexts.\n",
        "        - question_raws (list): A list of raw questions.\n",
        "    \"\"\"\n",
        "    contexts = []\n",
        "    context_raws = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    question_raws = []\n",
        "    for dataSquad in data:\n",
        "        for article in dataSquad['data']:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                context = clean_text(paragraph['context'])\n",
        "                for qa in paragraph['qas']:\n",
        "                    question = clean_text(qa['question'])\n",
        "                    # Do ngữ liệu nhiều chỗ define thiếu is_impossible nên sẽ mặc định là False\n",
        "                    is_impossible = qa.get('is_impossible', False)\n",
        "                    if not is_impossible:\n",
        "                        for answer in qa['answers']:\n",
        "                            answer_text = clean_text(answer['text'])\n",
        "                            answer_start = answer['answer_start']\n",
        "                            contexts.append(context)\n",
        "                            context_raws.append(paragraph['context'])\n",
        "                            questions.append(question)\n",
        "                            question_raws.append(qa['question'])\n",
        "                            answers.append({\n",
        "                                'text': answer_text,\n",
        "                                'start': answer_start\n",
        "                            })\n",
        "    return contexts, questions, answers, context_raws, question_raws"
      ],
      "metadata": {
        "id": "gxZvqtbkqjPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tokenize input"
      ],
      "metadata": {
        "id": "7FiUMnxMrEBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of texts using the word_tokenize function.\n",
        "\n",
        "    Args:\n",
        "      texts (list): A list of texts to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "      list: A list of tokenized texts.\n",
        "    \"\"\"\n",
        "    return [text.split() for text in texts]"
      ],
      "metadata": {
        "id": "rPV0RxnXrHKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Running preprocessing data"
      ],
      "metadata": {
        "id": "w1PjIAtwrcJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/data/qa_train.json'\n",
        "squad_data = load_all_data(data_path)\n",
        "contexts, questions, answers, contextRaws, questionRaws = preprocess_data(squad_data)\n",
        "tokenized_contexts = tokenize_texts(contexts)\n",
        "tokenized_questions = tokenize_texts(questions)"
      ],
      "metadata": {
        "id": "dhlIe_U0rbyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data\n",
        "# tokenized_questions[:100]"
      ],
      "metadata": {
        "id": "wnB0RBdIrl4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2vec model"
      ],
      "metadata": {
        "id": "k7MQB-jvrxVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install lib for model word2vec"
      ],
      "metadata": {
        "id": "yF8q3qMMr709"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "BDvpvBiar2gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Lib import using word2vec"
      ],
      "metadata": {
        "id": "gtHB5OPmsEwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "F5D3tirXsLFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create args model word2vec, build up lib for model and training"
      ],
      "metadata": {
        "id": "EWT58ydBsNzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_word2vec_question = Word2Vec(vector_size=100, window=10, min_count=1, sg=1, workers=4)\n",
        "model_word2vec_question.build_vocab(tokenized_questions)\n",
        "model_word2vec_question.train(tokenized_questions, total_examples=model_word2vec_question.corpus_count, epochs=200)\n",
        "model_word2vec_question.save(\"/content/model_word2vec_question.model\")"
      ],
      "metadata": {
        "id": "zpOHKKvXsbus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm BM25 (model BM25)**bold text**"
      ],
      "metadata": {
        "id": "M5-ppA9dtQT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install lib for model BM25"
      ],
      "metadata": {
        "id": "PmYXyuxotb-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "smCXZDsftj9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import lib using BM25"
      ],
      "metadata": {
        "id": "uyJBfG7ft6Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "Uz7Owvqyt9Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create model BM25"
      ],
      "metadata": {
        "id": "Tdf8nxz7tuuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25Questions = BM25Okapi(tokenized_questions)"
      ],
      "metadata": {
        "id": "3ew26_dmt1rO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}